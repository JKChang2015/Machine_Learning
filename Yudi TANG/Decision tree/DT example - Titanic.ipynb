{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"titanic_train.csv\")\n",
    "print(titanic.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve NaN in Age\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# numberical sex\n",
    "print(titanic['Sex'].unique())\n",
    "titanic.loc[titanic['Sex'] == 'male', 'Sex'] = 0\n",
    "titanic.loc[titanic['Sex'] == 'female', 'Sex'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# numberical Embarked\n",
    "print (titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null int64\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null int64\n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkchang/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The patterns use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "predictions =[]\n",
    "\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.261503928171\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "titanic_test = pd.read_csv(\"test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785634118967\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814814814815\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Mlle          2\n",
      "Major         2\n",
      "Col           2\n",
      "Lady          1\n",
      "Mme           1\n",
      "Jonkheer      1\n",
      "Ms            1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Countess      1\n",
      "Don           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEyCAYAAADqYisiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHf5JREFUeJzt3XuYZVV95vHvS7cIgsitwJZLGrFBUbnZQRCfqOAFJFxUEPDWYzBtnscoihMH4gwk3oJJ1BgcE3tE0hMBucmAIAppAUUFbO4gMCgiMCA0CnITDfDOH2uf7uqmus+p6tr7VK1+P89TT529z6n+reqqes/aa6+9tmwTERHT31rDbkBEREyOBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJmV0W23TTTT179uwuS0ZETHtXXXXVA7ZH+r2u00CfPXs2ixcv7rJkRMS0J+mXg7wuQy4REZVIoEdEVKJvoEvaXtK1oz4elvRhSRtLukjSbc3njbpocEREjK1voNu+1fbOtncGXgE8DpwNHA0ssj0HWNRsR0TEkIx3yGVv4Oe2fwkcCCxs9i8EDprMhkVExPiMN9APA05tHm9u+16A5vNmY32BpPmSFktavGTJkom3NCIiVmngQJe0NnAAcMZ4CtheYHuu7bkjI32nUUZExASNp4e+L3C17fua7fskzQJoPt8/2Y2LiIjBjSfQD2fZcAvAucC85vE84JzJalRERIzfQFeKSnoO8Abg/aN2Hw+cLukI4E7gkMlv3tQw++jzW69xx/H7tV4jIuo2UKDbfhzYZIV9v6bMeomIiCkgV4pGRFQigR4RUYkEekREJRLoERGVSKBHRFQigR4RUYkEekREJRLoERGVSKBHRFQigR4RUYkEekREJRLoERGVSKBHRFQigR4RUYkEekREJRLoERGVSKBHRFQigR4RUYkEekREJRLoERGVSKBHRFRioECXtKGkMyXdIulmSXtI2ljSRZJuaz5v1HZjIyJi5QbtoX8R+I7tFwM7ATcDRwOLbM8BFjXbERExJH0DXdIGwJ8AJwLY/oPth4ADgYXNyxYCB7XVyIiI6G+QHvoLgSXASZKukfRVSesBm9u+F6D5vNlYXyxpvqTFkhYvWbJk0hoeERHLGyTQZwK7Av9iexfgMcYxvGJ7ge25tueOjIxMsJkREdHPIIF+N3C37Sua7TMpAX+fpFkAzef722liREQMom+g2/4VcJek7ZtdewM/Bc4F5jX75gHntNLCiIgYyMwBX/dB4GRJawO3A++lvBmcLukI4E7gkHaaGBERgxgo0G1fC8wd46m9J7c5ERExUblSNCKiEgn0iIhKJNAjIiqRQI+IqEQCPSKiEgn0iIhKJNAjIiqRQI+IqEQCPSKiEgn0iIhKJNAjIiqRQI+IqEQCPSKiEgn0iIhKJNAjIiqRQI+IqEQCPSKiEgn0iIhKJNAjIiqRQI+IqEQCPSKiEjMHeZGkO4BHgKeAJ23PlbQxcBowG7gDeLvtB9tpZkRE9DOeHvrrbO9se26zfTSwyPYcYFGzHRERQ7I6Qy4HAgubxwuBg1a/ORERMVGDBrqBCyVdJWl+s29z2/cCNJ83G+sLJc2XtFjS4iVLlqx+iyMiYkwDjaEDe9q+R9JmwEWSbhm0gO0FwAKAuXPnegJtjIiIAQzUQ7d9T/P5fuBsYDfgPkmzAJrP97fVyIiI6K9voEtaT9Jze4+BNwI3AucC85qXzQPOaauRERHR3yBDLpsDZ0vqvf4U29+R9BPgdElHAHcCh7TXzIiI6KdvoNu+HdhpjP2/BvZuo1ERETF+uVI0IqISCfSIiEok0CMiKpFAj4ioRAI9IqISCfSIiEok0CMiKpFAj4ioRAI9IqISCfSIiEok0CMiKpFAj4ioRAI9IqISCfSIiEok0CMiKpFAj4ioRAI9IqISCfSIiEok0CMiKpFAj4ioRAI9IqISAwe6pBmSrpF0XrO9jaQrJN0m6TRJa7fXzIiI6Gc8PfQjgZtHbX8W+ILtOcCDwBGT2bCIiBifgQJd0pbAfsBXm20BewFnNi9ZCBzURgMjImIwg/bQ/wn4GPB0s70J8JDtJ5vtu4EtxvpCSfMlLZa0eMmSJavV2IiIWLm+gS7pT4H7bV81evcYL/VYX297ge25tueOjIxMsJkREdHPzAFesydwgKQ3A+sAG1B67BtKmtn00rcE7mmvmRER0U/fHrrtY2xvaXs2cBjwPdvvBC4GDm5eNg84p7VWRkREX6szD/2/AUdJ+hllTP3EyWlSRERMxCBDLkvZvgS4pHl8O7Db5DcpIiImIleKRkRUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUom+gS1pH0pWSrpN0k6S/bfZvI+kKSbdJOk3S2u03NyIiVmaQHvrvgb1s7wTsDOwjaXfgs8AXbM8BHgSOaK+ZERHRT99Ad/Fos/ms5sPAXsCZzf6FwEGttDAiIgYy0Bi6pBmSrgXuBy4Cfg48ZPvJ5iV3A1u008SIiBjEQIFu+ynbOwNbArsBLxnrZWN9raT5khZLWrxkyZKJtzQiIlZpXLNcbD8EXALsDmwoaWbz1JbAPSv5mgW259qeOzIysjptjYiIVRhklsuIpA2bx+sCrwduBi4GDm5eNg84p61GRkREfzP7v4RZwEJJMyhvAKfbPk/ST4FvSPoUcA1wYovtjIiIPvoGuu3rgV3G2H87ZTw9IiKmgFwpGhFRiQR6REQlEugREZVIoEdEVGKQWS4REa2bffT5rf77dxy/X6v//lSQHnpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlZg20xbbntIEa8a0poioV3roERGVSKBHRFRi2gy5RES0pZarVNNDj4ioRAI9IqISCfSIiEok0CMiKpFAj4ioRAI9IqISfQNd0laSLpZ0s6SbJB3Z7N9Y0kWSbms+b9R+cyMiYmUG6aE/CXzU9kuA3YEPSNoBOBpYZHsOsKjZjoiIIekb6LbvtX118/gR4GZgC+BAYGHzsoXAQW01MiIi+hvXGLqk2cAuwBXA5rbvhRL6wGaT3biIiBjcwIEuaX3gLODDth8ex9fNl7RY0uIlS5ZMpI0RETGAgQJd0rMoYX6y7W82u++TNKt5fhZw/1hfa3uB7bm2546MjExGmyMiYgyDzHIRcCJws+3Pj3rqXGBe83gecM7kNy8iIgY1yGqLewLvBm6QdG2z76+B44HTJR0B3Akc0k4TIyJiEH0D3fZlgFby9N6T25yIiJioXCkaEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUYpDVFiPWKLOPPr/1Gnccv1/rNWLNkx56REQlEugREZVIoEdEVCKBHhFRiQR6REQlEugREZVIoEdEVCKBHhFRib6BLulrku6XdOOofRtLukjSbc3njdptZkRE9DNID/3fgH1W2Hc0sMj2HGBRsx0REUPUN9Btfx/4zQq7DwQWNo8XAgdNcrsiImKcJjqGvrntewGaz5tNXpMiImIiWj8pKmm+pMWSFi9ZsqTtchERa6yJBvp9kmYBNJ/vX9kLbS+wPdf23JGRkQmWi4iIfiYa6OcC85rH84BzJqc5ERExUYNMWzwV+DGwvaS7JR0BHA+8QdJtwBua7YiIGKK+N7iwffhKntp7ktsSERGrIVeKRkRUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERlUigR0RUIoEeEVGJBHpERCUS6BERleh7g4uI6M7so89vvcYdx+/Xeo0YjvTQIyIqkUCPiKhEhlxiSsrQQ8T4pYceEVGJ1eqhS9oH+CIwA/iq7eMnpVWxVHqqETGoCffQJc0A/iewL7ADcLikHSarYRERMT6r00PfDfiZ7dsBJH0DOBD46WQ0LIYvRwcR08vqBPoWwF2jtu8GXrl6zYmIYckb+PQn2xP7QukQ4E2239dsvxvYzfYHV3jdfGB+s7k9cOvEmzsumwIPdFRrqtVP7dRO7bpq/5HtkX4vWp0e+t3AVqO2twTuWfFFthcAC1ajzoRIWmx7btd1p0L91E7t1K639qqszrTFnwBzJG0jaW3gMODcyWlWRESM14R76LaflPSXwHcp0xa/ZvumSWtZRESMy2rNQ7f9beDbk9SWydb5MM8Uqp/aqZ3a9dZeqQmfFI2IiKkll/5HRFQigR4RUYkEekRMG5LWlbT9sNsxVVUV6JK2lfTs5vFrJX1I0oYd1f6kpJmjtjeQdFIXtYdN0vMlHSBpf0nPH0L9LSS9StKf9D46qitJ75J0bLO9taTduqi9JpK0P3At8J1me2dJnU6VljRD0guan/XWkrbusn4/ta2HfhYwV9KLgBMp8+JPAd7cQe2ZwBWS3gs8Hzih+WidpM2BzwAvsL1vs0jaHrZP7KD2+4Bjge8BAk6Q9AnbX2u7dlP/s8ChlDWEnmp2G/h+B+W/DDwN7AV8AniE8jv4x20VlPQtyvc3JtsHtFV7VBu2A/4F2Nz2yyTtCBxg+1Mtl/4byhpSlwDYvlbS7JZrLiXpg8BxwH2UnzuUn8WOXbWhL9vVfABXN5//Cvhg8/iaDuu/Hvgd5YrZF3VY9wLg7cB1zfZM4IaOat8KbDJqexPg1g6/91uBZ3dVb4Xavd+3a0btu67lmq9pPr4InAbs33ycAnymo+/7Ukqwjv6+b+yg7hVj/H9f3+HP+2ejf9en4kdVQy7Af0o6HJgHnNfse1YXhZvD/C9SemqXAF+S9IIuagOb2j6dptdg+0mW9VbbdjelZ9rzCMsv2ta22+noZzyG/2yWkTaApBGW9dxaYftS25cCu9g+1Pa3mo93AK9us/Yoz7F95Qr7nuyg7o2S3gHMkDRH0gnAjzqo23MX8NsO641bbUMu7wX+Avi07V9I2gb4eke1/xE4xPZPASS9lTIM8eIOaj8maROWBcvudPeL9/8oQ03nNPUPBK6UdBSA7c+3UbT5YzbwOHCtpEXA73vP2/5QG3VX8M/A2cBmkj4NHAz89w7qAoxIeqGXLV+9DdB38aZJ8oCkbVn2+3YwcG8HdT8IfJzycz6VcpX6J9su2vtdpnQeLpF0Psv/rrXyOz4R1V5YJGkjYCvb13dUb4btp1bYt4ntX3dQe1fKeP3LgBspf9gHd/G9SzpuVc/b/tuW6s7rU3dhG3XHaMeLgb0p5w8W2b65o7r7UK5WvL3ZNRt4v+3vdlD7hU3tVwEPAr8A3mX7jrZrD0Of33Hb/kRnjemjqkCXdAlwAOXI41pgCXCp7aNW9XWTVLt3YnIL2/t0eWKyqT+TsjyxKGPY/9lF3RXasBHwkDv8pZK0HvBE7820GQJ5tu3HW667FmX89mVt1unThmez7AjwFtu/X9XrW6i/HrCW7Uf6vnj16gz9RHDTjkNsn9Fv3zDVFujX2N6lmXmxle3jJF1vu/Wz0JIuAE4CPm57pyZgr7H98g5qv3WM3b+lnBi9v6WaxwKn276lCZYLgJ0pY6nvsP0fbdQdox2XA6+3/WizvT5woe1XdVD7ZOAY23e2XWuM2s8BjqKsk/3nkuYA29s+r8+XTkbtp4B/oHzvvWGXq23v2lK916zq+eacQuvG+h7b/L4norYx9JmSZlFmfHy849qb2j5d0jGwdDXKrk5MHgHsAVzcbL8WuBzYrplC+O8t1DyUZeOX8yjXNIwA2wELgU4CHVinF+YAth9twq4Ls4CbJF0JPDaqDV30GE8CrqL83KGcnD6DZZMB2nQT5ed9oaRDbf+GcmTYil5gSzrS9hdHPyfpSMqsm9ZI2pcy9XkLSf886qkN6OZk8MBqC/RPUE6UXGb7J81Y320d1R7micmngZfYvq+pvTllnvArKfOx2wj0P4waWnkTcGoz7HHz6AusOvCYpF1tXw0g6RWUqaNdaOX8wIC2tX1oM6sL27+T1FqoruBJ2x+T9HbgB5LewyqGRCbRPMpMstH+yxj7Jts9wGLKcO5Vo/Y/Anyk5drjUlWgN2NZZ4zavh14W0flj6JcyLStpB/SnJjsqPbsXpg37ge2s/0bSW2Npf9e0ssoF1m8Dvivo57rqocMcCRwhqTe3bJmUY4eWtfVof5K/EHSuizrQGzLqJkXLRNAc0R6E2XGSWtXTDZvWu8AtlnhytDnAq1POrB9HXCdpFOGcW5qPKoKdEnrUIYfXgqs09tv+89arPnHwF22r27G+t5PeRO5kHIY3IUfSDqPZW9mbwO+35y0eqilmkcCZ1LeuL5g+xcAkt4MXNNSzeU0JybXppwY7J0QvqWrP7rmKOwE4CVNO2YAj9neoIPyx1Eugd+qGcvfk9Jb7cL7eg9s3yTp1cBBLdb7EWVa5KbA50btfwToZBZb42pJKx6J/JbSe/9UFzPa+qntpOgZwC2Ud/NPAO8EbrZ9ZIs1r6aclPtNc3HRNyjzZXemDIO03ktvDrXfyrILS34NzLL9gbZrD5ukH9veo/8rW6m9mHLrxTOAucB7gDm2/7qj+psAu1PeyC633epNiyXtZft7KzkJj+1vtll/2CT9PeWCvVOaXYdR/u9/C7za9v7DaltPVT10yuX2h0g60PZCSadQxtTbNKM5KQTlUH+B7bOAsyRd23JtoEyElfRzypj52ynzgs/qonYTKsdR3kwMXAZ8osPeyoWS3gZ8s8vpkj22fzbqGoSTJHVy5WJzsvtY4Pxmey1JJ9t+Z4tlX0O5WG6s4DLQSqBLusz2qyU9wvJj9aL8+ndxRASwp+09R23fIOmHtveU9K6O2rBKtQV671D7oWZ891eUCy7aNEPSzOZy+72B+aOea/X/V2WRpMOAwym98tMoR12va7PuCr5BOfHaO1fxzqYdr++o/lHAesCTkp6g2z/yx1VukH5t03u7t2lLF7aWdIztv2umjZ4BXN1mQdvHNZ/f22adMazX1H1ux3VXtL6kV9q+AkBlZc31m+emxGyX2oZc3kfpme5Imda1PnCs7X9tsebHKVOaHqCcGNq16TG/CFi4wjv6ZNd+GvgBcITtnzX7brf9wrZqjtGGq2y/YoV9i23P7aoNwyLpjygnhdemzHZ4HvDl3s+i5doCTgZuoJyUvsD2F1quuT/lYqpfNtvHUt7Ifwkc2TuP0kLdKTHXuzlf9jVKrgh4mHI+4SZgP5f1lIaqqkAflubk2CzKBS2PNfu2A9bvTadrqe5bKD30V1FOkH0D+KrtbdqqOUYb/pFyUqj3y3ww8NJeb66jNmwEzGH5E+GtLZ8raethXEzU1B4dbM8CvgL8kLJcNC3/vl0P7G77cUl/CnyecnS4C2Udoze1VPfuptaY3PFaKpKeR8nOtiYcTFgVga5li+eMqesfeNea2SwHUf649qJc2HO27QtbrNkbzxTlkLh3EdUM4NGuxjWbo7IjgS0pyz3sDvzY9l4t1lzaY5R0lu2upsYi6eJVPO2Wv+/rbO/UPP4aZYmJzzbbbV4pei/luoox59m7pfWCxmjHsylHJLMZNZzqKbSWSy1j6MMeWxuq5qjgZOBkSRsDhwBHU6ZOtlVzqvyfH0m5ocTltl+nslhW23/go4Ols+EtgOZ7XIvSIz6ty9qUkZ71KStc7k25wUfPOmN/yaS4d4qE5jmUGS1X0d2c/3GpItC7eoeeDpoZN19pPloj6cUu67iM2Str89B/BU/YfkISkp7dtKnte056JY87YftpSR+gnHzu0j9RjoIepkwHXgwgaRfaXT63qytg+9nS9j7DbsSqVDHk0iNpIeXkzEPN9kbA59q8sGhNJWmB7fkrDAEs/WVq89B/hXacTVkH/8OU4aYHgWfZbu22gypr9DxGCZp1KT1W6HCGjaT/QVni4DSWX0fmNyv9osmpuwWwGeXOTE83+2ZR/s9bOa8gaeO2v68B27EAOMH2DcNuy8rUFujX2N6l375Yfc2UrTtt/6rZnkcZX7wD+Jth/AE2V+o+D/iO7T90Xb9LksaaUeIuZjhJOpMy2+M7vVBfE0j6KfAiynUev2fZG/iUuadobYF+HfBa2w822xtT1kNvfQnbNc2wr5Btlnn4C8of2A3Aic21ANEySa+nHBXtTpn//m+2bxluq9rXTFN9ht40zqmgijH0UT4H/LhZAsCUqyY/PdwmVWvYV8gupFxI9gNgX2AHygnSNUZz8dwOLD9d83+3Xddlrfv/aKbvHQ5cJOku4H8BX/cUX8Bqomz/slm3Zo7tk1TuIbt+v6/rUlU9dACVOwXtBUtvCfbTITepSpJuBHZ2Wff9FmB+b+63pBvd8p18JN3QO/JSWa73yqlw8UlXVG6L9lpKoH+b8qZ2WdtHRqPqbwK8C3g3ZXnZkynLP7zc9mu7aEPXmv/zuZQbiWynchP4M9q8eHC8quihj3H4/a85/G7dqcClkh6gnJz7AUBzhWwX68Av7QU2byodlJxSDgZ2otwV670qa+B/tYvCkr5JWeHy34H9bfdmuJymsmBZrd5CuYjqagDb90iaKtN3gUoCnWcefr+EMushWmL705IWsewK2d6h3lqUsfS27STp4eaxgHWb7a4XbBqW3zXTF5+UtAFlDfyu5sR/yfb3xnqi8iUf/tAs69Fbg76rdXsGVkug7zDq8PtE4Moht2eNYPvyMfb9345qz+iizhS2WNKGlHHrq4BHafn3XqOWzdUYS+i68uVzgdMlfQXYUNKfA39GR0dFg6piDH3Fy46nymI+EV2QNBvYwHarN3uQdNIqnvaacL2HpDcAb6QcCX7X9kVDbtJyagn03oUesPzFHmvK4XesgZpe8tJ16G2fPeQmrXHUrIc+7Hb0VBHoEWsaSV+mTAI4tdl1KPBzt3iXKknvsv31lS2GV/sieGORdJftrYbdjp5axtAj1jSvAV7WOxndLHvR9iXpvZOAU2pmx5BNqR5xAj1ierqVckOV3lWKW9HyDZNtf6X5vEYthjfWCeDeU5Th3SkjgR4xjUj6FqVX+DzgZklXNtuvBLq6n+k2lKmps1l+XfADuqg/BKu6+fN5nbViABlDj5hGmgXIVsr2pR204TrKHZJuAJYuztVF7Vi1BHrENNZcVDS6l9z6KpeSrrD9yrbrTDXN1bifAV5ge99mmZE9bJ845KYtlUCPmIYkzQc+SVl24WmWTdHtYvncd1Du4Xoho+7c0+FNTYZC0gWUm89/3PZOzRpC10yl1Vwzhh4xPf0V5WbcDwyh9sspi3LtxbIhFzfbNdvU9umSjoGlawg91e+LupRAj5iefs6yOyV17S3AC2u/icgYHmtWmexNFd2dbhaiG1gCPWJ6Ogb4kaQrWH7Y40Md1L4O2JCyINia5CjgXGBbST8ERiirXk4ZGUOPmIaa6YqX8cyZJgs7qH0JsCPwE5Z/M6l12uJSzbj59pRzFrdOtZt5JNAjpiFJP7L9qiHVHnPqZO3TFiXNAPbjmfPvp8ySBxlyiZieLm5munyL5XvJrU9brD24V+FbwBOscFQ0laSHHjENSfrFGLu7mra4O3AC5UYyawMzgMdqX9VU0vW2dxx2O1YlPfSIacj2NkMs/yXgMOAMyj0230OZl167CyS90faFw27Iyqw17AZExOAkfWzU40NWeO4zXbXD9s+AGbafsn0S5YbVtbscOFvS7yQ9LOmRUbdBnBIS6BHTy2GjHh+zwnP7dNSGxyWtDVwr6e8lfYRlS+vW7HPAHsBzbG9g+7lTbZgpgR4xvWglj8fabsu7Kdnxl5Q7hW0FvK2j2sN0G3Cjp/CJx4yhR0wvXsnjsbYnlaStbd9pu7cG+xPAmrQ2+r3AJc2aLqNnFmXaYkRMyE7NuK2AdUeN4QpYp+Xa/wfYFUDSWbbXhF75aL9oPtZuPqacBHrENGJ7xhDLjx7SaX165FQzHe7UlECPiEGtarinepJGgI8BL2XU0ZDtKbPKZE6KRsSgdupN1wN2bB5Pyel7LTkZuAXYhnLu4A7KejZTRq4UjYgYgKSrbL9i9BWjki61vcrbAnYpQy4REYPprax4r6T9gHuALYfYnmdIoEdEDOZTkp4HfJSyls0GwEeG26TlZcglIqIS6aFHRKyCpGNX8bRtf7KzxvSRHnpExCpI+ugYu9cDjgA2sb1+x01aqQR6RMSAJD0XOJIS5qcDn7M9Ze6tmiGXiIg+JG1MuUn0O4GFwK62Hxxuq54pgR4RsQqS/gF4K7AAeLntR4fcpJXKkEtExCpIepqyuuKTLL/kgSgnRafMmugJ9IiISmQtl4iISiTQIyIqkUCPiKhEAj0iohIJ9IiISvx/2xq0wG/7e9kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.279461279461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\",]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pd.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11681943,  0.47836987,  0.12614048,  0.13097708,  0.52107272,\n",
       "        0.14351509,  0.64086375,  0.18002627,  0.67802231,  0.12110664,\n",
       "        0.12104744,  0.20901962,  0.91068965,  0.10891017,  0.89143306,\n",
       "        0.87714269,  0.16348534,  0.13906931,  0.54105377,  0.55662058,\n",
       "        0.22420124,  0.53718352,  0.90572929,  0.38889821,  0.88385196,\n",
       "        0.10357086,  0.90910138,  0.13745621,  0.31046113,  0.1266512 ,\n",
       "        0.11663426,  0.18274307,  0.55222145,  0.49649779,  0.42414476,\n",
       "        0.14190148,  0.5097587 ,  0.52454348,  0.13270036,  0.28366139,\n",
       "        0.11144781,  0.46618779,  0.0999618 ,  0.83421925,  0.89960308,\n",
       "        0.14982951,  0.31592763,  0.13788495,  0.89105091,  0.54190897,\n",
       "        0.3566613 ,  0.17717223,  0.83074006,  0.87996462,  0.17558166,\n",
       "        0.13738875,  0.10666907,  0.12343384,  0.12099323,  0.9128551 ,\n",
       "        0.13098613,  0.15341529,  0.12993431,  0.66574184,  0.66341763,\n",
       "        0.87273821,  0.67239645,  0.28826312,  0.35235967,  0.85566511,\n",
       "        0.66225137,  0.12701486,  0.55392305,  0.36739876,  0.91110735,\n",
       "        0.41201027,  0.13013567,  0.83673046,  0.15613989,  0.66225137,\n",
       "        0.68126047,  0.20605054,  0.20382454,  0.12104744,  0.18485201,\n",
       "        0.13129541,  0.65681561,  0.53031943,  0.65490661,  0.7987881 ,\n",
       "        0.53765995,  0.12103592,  0.89138279,  0.13013567,  0.28405691,\n",
       "        0.12344901,  0.86794385,  0.14665909,  0.58601586,  0.12260391,\n",
       "        0.90434217,  0.14730313,  0.13788495,  0.12261977,  0.62258308,\n",
       "        0.13155404,  0.14606445,  0.13788495,  0.13019897,  0.1747259 ,\n",
       "        0.14285637,  0.65491346,  0.89529098,  0.67147699,  0.88346925,\n",
       "        0.13991227,  0.11804329,  0.69614332,  0.36668206,  0.86243053,\n",
       "        0.87650636,  0.12608344,  0.90276979,  0.12098591,  0.13788495,\n",
       "        0.56973992,  0.12607685,  0.6373454 ,  0.13339624,  0.13340097,\n",
       "        0.12723238,  0.516065  ,  0.23922865,  0.10791048,  0.09896431,\n",
       "        0.12430648,  0.13345732,  0.16213663,  0.52031607,  0.12232514,\n",
       "        0.20713034,  0.90530415,  0.19746623,  0.16153256,  0.42927458,\n",
       "        0.10486884,  0.33642421,  0.13517918,  0.46618779,  0.34475031,\n",
       "        0.91431763,  0.13214259,  0.106908  ,  0.48984982,  0.11273495,\n",
       "        0.12427392,  0.91070653,  0.57993806,  0.42927458,  0.51275443,\n",
       "        0.65490269,  0.5788139 ,  0.82115224,  0.12096213,  0.2897616 ,\n",
       "        0.58588482,  0.30129764,  0.14606414,  0.90250897,  0.52259532,\n",
       "        0.12101447,  0.13298743,  0.12418074,  0.13206749,  0.13196412,\n",
       "        0.8729528 ,  0.8763491 ,  0.2966958 ,  0.83391074,  0.85559817,\n",
       "        0.15613989,  0.33351255,  0.90219659,  0.13788495,  0.91719144,\n",
       "        0.13602615,  0.85484209,  0.12240939,  0.14217439,  0.13560305,\n",
       "        0.13487572,  0.25547156,  0.4994872 ,  0.12728693,  0.71978347,\n",
       "        0.10795079,  0.855151  ,  0.58992535,  0.16645233,  0.53981907,\n",
       "        0.64868974,  0.66326561,  0.60979493,  0.87334866,  0.16322206,\n",
       "        0.25696069,  0.63084589,  0.16482157,  0.88985628,  0.12345941,\n",
       "        0.12849223,  0.12096689,  0.24674446,  0.80201864,  0.41248946,\n",
       "        0.29767987,  0.65493693,  0.21859743,  0.90027904,  0.13013567,\n",
       "        0.81371562,  0.13610635,  0.84276502,  0.12700322,  0.87790232,\n",
       "        0.59808804,  0.12517601,  0.65490661,  0.11487155,  0.14412709,\n",
       "        0.25074609,  0.89267223,  0.11622218,  0.13790202,  0.34223771,\n",
       "        0.12796256,  0.19365149,  0.14018024,  0.80950131,  0.89791511,\n",
       "        0.87599955,  0.82599874,  0.33035454,  0.12104665,  0.33256695,\n",
       "        0.2871044 ,  0.87904012,  0.16058594,  0.86243053,  0.59134008,\n",
       "        0.74587991,  0.1543381 ,  0.39646483,  0.13353789,  0.12701466,\n",
       "        0.12101447,  0.13788495,  0.13013567,  0.83007385,  0.12700079,\n",
       "        0.10894619,  0.12701002,  0.85005234,  0.64931714,  0.16618664,\n",
       "        0.12104744,  0.21821031,  0.12101447,  0.5097587 ,  0.14015932,\n",
       "        0.3449509 ,  0.13788495,  0.91564324,  0.63329198,  0.13206702,\n",
       "        0.85715289,  0.15861211,  0.12499702,  0.14266702,  0.16811417,\n",
       "        0.52047246,  0.66229245,  0.65490661,  0.64138125,  0.71200672,\n",
       "        0.10600723,  0.12098591,  0.36277807,  0.13206749,  0.13013567,\n",
       "        0.33304406,  0.59320635,  0.13206749,  0.5058149 ,  0.1208131 ,\n",
       "        0.12263198,  0.77904145,  0.1266512 ,  0.33024405,  0.12028548,\n",
       "        0.11813558,  0.17546984,  0.12169028,  0.13346667,  0.65490661,\n",
       "        0.82135602,  0.33496789,  0.67693366,  0.20916067,  0.42576549,\n",
       "        0.1391233 ,  0.13798687,  0.12101686,  0.61905813,  0.90112575,\n",
       "        0.67394569,  0.23918442,  0.17328368,  0.12182407,  0.18522385,\n",
       "        0.12261977,  0.13490689,  0.16213663,  0.45541235,  0.9060203 ,\n",
       "        0.12509399,  0.8656554 ,  0.34597795,  0.14469227,  0.17033775,\n",
       "        0.82149328,  0.32822924,  0.13206702,  0.64323718,  0.12182816,\n",
       "        0.25111353,  0.15333007,  0.09369676,  0.20950322,  0.35409118,\n",
       "        0.1750671 ,  0.11811901,  0.14695545,  0.91556576,  0.33656009,\n",
       "        0.61838844,  0.16213663,  0.6246373 ,  0.16542449,  0.85159855,\n",
       "        0.89604589,  0.16322206,  0.24472223,  0.16066254,  0.70032835,\n",
       "        0.15642285,  0.85674382,  0.12104585,  0.13788495,  0.57256735,\n",
       "        0.10418161,  0.87673302,  0.86920135,  0.13097708,  0.9191463 ,\n",
       "        0.15714899,  0.13129579,  0.53324136,  0.89563745,  0.17355165,\n",
       "        0.15319342,  0.90892039,  0.16307814,  0.13130214,  0.87656306,\n",
       "        0.90969631,  0.48855368,  0.17001886,  0.19866738,  0.13509336,\n",
       "        0.13788495,  0.14009086,  0.54135379,  0.59500647,  0.15905205,\n",
       "        0.83278804,  0.124298  ,  0.12019101,  0.14605329,  0.18787717,\n",
       "        0.38579213,  0.87751493,  0.56456941,  0.128075  ,  0.10317864,\n",
       "        0.91170132,  0.14230296,  0.8877404 ,  0.1260745 ,  0.12970092,\n",
       "        0.90754457,  0.12634745,  0.90892342,  0.35988753,  0.30441689,\n",
       "        0.18965844,  0.15014721,  0.26821068,  0.65489976,  0.64587182,\n",
       "        0.65490661,  0.90712204,  0.56935712,  0.13013567,  0.86010034,\n",
       "        0.10126334,  0.13013567,  0.41848035])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
