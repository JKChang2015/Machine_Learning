
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Neural Network}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Artificial Neural Network}\label{artificial-neural-network}

    Artificial neural networks (ANNs) are computing systems vaguely inspired
by the biological neural networks that constitute animal brains. Such
systems "learn" tasks by considering examples, generally without
task-specific programming. For example, in image recognition, they might
learn to identify images that contain cats by analyzing example images
that have been manually labeled as "cat" or "no cat" and using the
results to identify cats in other images. They do this without any a
priori knowledge about cats, e.g., that they have fur, tails, whiskers
and cat-like faces. Instead, they evolve their own set of relevant
characteristics from the learning material that they process.

An ANN is based on a collection of connected units or nodes called
artificial neurons. Each connection (a simplified version of a synapse)
between artificial neurons can transmit a signal from one to another.
The artificial neuron that receives the signal can process it and then
signal artificial neurons connected to it.

In common ANN implementations, the signal at a connection between
artificial neurons is a real number, and the output of each artificial
neuron is calculated by a non-linear function of the sum of its inputs.
Artificial neurons and connections typically have a weight that adjusts
as learning proceeds. The weight increases or decreases the strength of
the signal at a connection. Artificial neurons may have a threshold such
that only if the aggregate signal crosses that threshold is the signal
sent. Typically, artificial neurons are organized in layers. Different
layers may perform different kinds of transformations on their inputs.
Signals travel from the first (input), to the last (output) layer,
possibly after traversing the layers multiple times.

CIFAR-10: https://www.cs.toronto.edu/\textasciitilde{}kriz/cifar.html 10
tags, 50,000 training data, 10,000 testing data, size are 32*32 Tags:
airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck

    \subsubsection{Linear loss function}\label{linear-loss-function}

Linear classification function: \[f(x,W) = Wx+b\] Linear Loss function:
\[L = \frac{1}{N}\sum_{i=1}^{N}\sum_{j\neq y_{i}}^{ } max(0,f(x_{i};W)_{j} -f(x_{i};W)_{y_{i}} + 1 )\]
Average value of all sample training data

    \subsubsection{Penalty function}\label{penalty-function}

Penalty methods are a certain class of algorithms for solving
constrained optimization problems.

A penalty method replaces a constrained optimization problem by a series
of unconstrained problems whose solutions ideally converge to the
solution of the original constrained problem. The unconstrained problems
are formed by adding a term, called a penalty function, to the objective
function that consists of a penalty parameter multiplied by a measure of
violation of the constraints. The measure of violation is nonzero when
the constraints are violated and is zero in the region where constraints
are not violated.

\[L = \frac{1}{N}\sum_{i=1}^{N}\sum_{j\neq y_{i}}^{ } max(0,f(x_{i};W)_{j} -f(x_{i};W)_{y_{i}} + 1 ) + \lambda R(W)\]
where, R(W) is the regular pennalty (L2)
\[R(W) = \sum_{k}^{ }\sum_{l}^{ }w_{k,l}^{2}\]

    \subsubsection{Softmax}\label{softmax}

\begin{itemize}
\tightlist
\item
  SVM: Scoring result,
\item
  Softmax: Probability
\end{itemize}

In mathematics, the softmax function, or normalized exponential
function,is a generalization of the logistic function that "squashes" a
K-dimensional vector \texttt{z} of arbitrary real values to a
K-dimensional vector \(\sigma (z)\) of real values in the range (0, 1)
that add up to 1. The function is given by
\[\sigma :\mathbb {R} ^{K}\to (0,1)^{K}\]
\[\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}\]
where, j = 1, \ldots{}, K

In probability theory, the output of the softmax function can be used to
represent a categorical distribution -- that is, a probability
distribution over K different possible outcomes. In fact, it is the
gradient-log-normalizer of the categorical probability distribution.

 The softmax function is used in various multiclass classification
methods, such as multinomial logistic regression (also known as softmax
regression).multiclass linear discriminant analysis, naive Bayes
classifiers, and artificial neural networks.

 Specifically, in multinomial logistic regression and linear
discriminant analysis, the input to the function is the result of K
distinct linear functions, and the predicted probability for the j'th
class given a sample vector x and a weighting vector w is:

\[{\displaystyle P(y=j\mid \mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}}}\]

 This can be seen as the composition of K linear functions \$
\mathbf {x} \mapsto \mathbf {x} \^{}\{\mathsf {T}\}\mathbf {w}
\_\{1\},\ldots\$ ,
\(\mathbf {x} \mapsto \mathbf {x} ^{\mathsf {T}}\mathbf {w} _{K}\) and
the softmax function (where \(\mathbf {x} ^{\mathsf {T}}\mathbf {w}\)
denotes the inner product of \(\mathbf {x}\) and \(\mathbf {w}\) ). The
operation is equivalent to applying a linear operator defined by
\(\mathbf {w}\) to vectors \(\mathbf {x}\), thus transforming the
original, probably highly-dimensional, input to vectors in a
K-dimensional space \({\displaystyle \mathbb {R} ^{K}}\). \#\#\#\# loss
function for softmax: using correct answers to calculate the log value
\[L_{i} = log({\frac {e^{sz_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}})\]

    \subsubsection{Convolutional Neural Network Optimization
Algorithms}\label{convolutional-neural-network-optimization-algorithms}

On this picture we can detect the following components: 1. Training
dataset: Basically a high speed disk containing your training data 2.
Batch of samples: A list of pairs (X,Y), consisting on inputs, expected
outputs, for example X can be a image and Y the label "cat" 3.
Parameters: Set of parameters used by your model layers, to map X to Y
4. Model: Set of computing layers that transform an input X and weights
W, into a score (probable Y) 5. Loss function: Responsible to say how
far our score is from the ideal response Y, the output of the loss
function is a scalar. Another way is also to consider that the loss
function say how bad is your current set of parameters W.

 \texttt{forward-propagation:} from w to the result Loss function
\texttt{back-propagation:} from Loss function back to w (red line)

    \subsubsection{Back propagation}\label{back-propagation}

On the picture bellow we get a node f(x,y) that compute some function
with two inputs x,y and output z. Now on the right side, we have this
same node receiving from somewhere (loss function) a gradient dL/dz
which means. "How much L will change with a small change on z". As the
node has 2 inputs it will have 2 gradients. One showing how L will a
small change dx and the other showing how L will change with a small
change (dz)

 In order to calculate the gradients we need the input dL/dz (dout), and
the derivative of the function f(x,y), at that particular input, then we
just multiply them. Also we need the previous cached input, saved during
forward propagation. 

    \subsubsection{Back propagation
examples}\label{back-propagation-examples}

 \#\#\#\# simple example 1. Start from output node f, and consider that
the gradient of f related to some criteria is 1 (dout) 2. dq=(dout(1) *
z), which is -4 (How the output will change with a change in q) 3.
dz=(dout(1) * q), which is 3 (How the output will change with a change
in z) 4. The sum gate distribute it's input gradients, so dx=-4, dy=-4
(How the output will change with x,z)

\paragraph{Perceptron with 2 inputs}\label{perceptron-with-2-inputs}

This following graph represent the forward propagation of a simple 2
inputs, neural network with one output layer with sigmoid activation.
sigmoid function: \[\sigma (x) = \frac{1}{1+e^{-x}}\]
\[f(w,x) = \frac{1}{1+e^{-(w_{0}x_{0}+ w_{1}x_{1}+w_{2})}}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start from the output node, considering that or error(dout) is 1
\item
  The gradient of the input of the 1/x will be -1/(1.37\^{}2), -0.53
\item
  The increment node does not change the gradient on it's input, so it
  will be (-0.53 * 1), -0.53
\item
  The exp node input gradient will be (exp(-1(cached input)) * -0.53),
  -0.2
\item
  The negative gain node will be it's input gradient (-1 * -0.2), 0.2
\item
  The sum node will distribute the gradients, so, dw2=0.2, and the sum
  node also 0.2
\item
  The sum node again distribute the gradients so again 0.2
\item
  dw0 will be (0.2 * -1), -0.2
\item
  dx0 will be (0.2 * 2). 0.4
\end{enumerate}

    \subsubsection{Basic blocks}\label{basic-blocks}

 Some examples of basic blocks are, add, multiply, exp, max. All we need
to do is observe their forward and backward calculation: 

    \subsubsection{Artificial Neuron}\label{artificial-neuron}

 The single artificial neuron will do a dot product between w and x,
then add a bias, the result is passed to an activation function that
will add some non-linearity. The neural network will be formed by those
artificial neurons. The non-linearity will allow different variations of
an object of the same class to be learned separately. Which is a
different behaviour compared to the linear classifier that tries to
learn all different variations of the same class on a single set of
weights. More neurons and more layers is always better but it will need
more data to train. Each layer learn a concept, from it's previous
layer. So it's better to have deeper neural networks than a wide one.

\subsubsection{Neural networks as computation
graphs}\label{neural-networks-as-computation-graphs}

 

    \subsubsection{Activation Functions}\label{activation-functions}

 After the neuron do the dot product between it's inputs and weights, it
also apply a non-linearity on this result. This non-linear function is
called Activation Function.On the past the popular choice for activation
functions were the sigmoid and tanh. Recently it was observed the
\texttt{ReLU} layers has better response for deep neural networks, due
to a problem called vanishing gradient. So you can consider using only
ReLU neurons. 

    \subsubsection{Model Initialization}\label{model-initialization}

\begin{itemize}
\item
  If you initialize your weights to zero, your gradient descent will
  never converge 
\item
  Initialize with small values Here randn gives random data with zero
  mean, unit standard deviation. are the number of input and outputs.
  The 0.01 term will keep the random weights small and close to zero. 
\end{itemize}

The problem with the previous way to do initialization is that the
variance of the outputs will grow with the number of inputs. To solve
this issue we can divide the random term by the square root of the
number of inputs.

Now it seems that we don't have dead neurons, the only problem with this
approach is to use it with Relu neurons.

To solve this just add a simple (divide by 2) term.... 

    \subsubsection{Dropout}\label{dropout}

 Dropout is a technique used to improve over-fit on neural networks, you
should use Dropout along with other techniques like L2 Regularization.
Basically during training half of neurons on a particular layer will be
deactivated. This improve generalization because force your layer to
learn with different neurons the same "concept".During the prediction
phase the dropout is deactivated.

 Bellow we have a classification error (Not including loss), observe
that the test/validation error is smaller using dropout 


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
