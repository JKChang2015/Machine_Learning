{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers \"based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "\n",
    "$${\\mbox{posterior}}={\\frac {{\\mbox{prior}}\\times {\\mbox{likelihood}}}{\\mbox{evidence}}}$$\n",
    "<br/>\n",
    "\n",
    "Now the \"naive\" conditional independence assumptions come into play: assume that each feature $x_{i}$ is conditionally independent of every other feature $x_{j}$ for $j\\neq i$, given the category $C_{k}$. This means that:\n",
    "$${\\displaystyle p(x_{i}\\mid x_{i+1},\\dots ,x_{n},C_{k})=p(x_{i}\\mid C_{k})\\,}$$ <br/>\n",
    "Thus, the joint model can be expressed as:\n",
    "$${\\displaystyle {\\begin{aligned}p(C_{k}\\mid x_{1},\\dots ,x_{n})&\\varpropto p(C_{k},x_{1},\\dots ,x_{n})=\\\\&=p(C_{k})\\ p(x_{1}\\mid C_{k})\\ p(x_{2}\\mid C_{k})\\ p(x_{3}\\mid C_{k})\\ \\cdots \\\\&=p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})\\,.\\end{aligned}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label ${\\hat  {y}}=C_{k}$ for some k as follows:\n",
    "$${\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\ p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(x_{i}\\mid C_{k}).}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian naive Bayes\n",
    "When dealing with continuous data, a typical assumption is that the continuous values `associated` with each class are distributed according to a `Gaussian distribution`. For example, suppose the training data contains a continuous attribute, `x`. We first segment the data by the class, and then compute the mean and variance of `x` in each class. Let $\\mu _{k}$ be the mean of the values in x associated with class $C_{k}$, and let $\\sigma _{k}^{2}$ be the variance of the values in x associated with class Ck. Suppose we have collected some observation value v. Then, the probability distribution of v given a class $C_{k}$, $p(x=v\\mid C_{k})$, can be computed by plugging v into the equation for a Normal distribution parameterized by $\\mu _{k}$ and $\\sigma _{k}^{2}$: \n",
    "<br/>\n",
    "that is,\n",
    "$$p(x=v\\mid C_{k})={\\frac {1}{\\sqrt {2\\pi \\sigma _{k}^{2}}}}\\,e^{-{\\frac {(v-\\mu _{k})^{2}}{2\\sigma _{k}^{2}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
